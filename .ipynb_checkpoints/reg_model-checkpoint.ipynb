{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Résumé de tâche :\n",
    "\n",
    "**L’objectif** est : pour un prompt et deux réponses données R et R* où R* est une **« bonne réponse »**, est-ce que « R est aussi bonne que R* » ?\n",
    "> ⬆️ Comparaison\n",
    "\n",
    "Plus **précisément** : \n",
    "1. Ont-elles des points communs ?  `analyse`\n",
    "2. Des différences ? `analyse`\n",
    "3. Elles se complètent ? En les combinant, on obtiendrait une réponse encore meilleure ?  `génération+évaluation`\n",
    "\n",
    "\n",
    "\n",
    "> - Analyser le problème, proposer une solution et l’implémenter.\n",
    "> - Pas besoin de chercher à faire le meilleur modèle possible (ni de tester des dizaines de modèles)\n",
    "> - Se Concentrer sur l’analyse du problème, la proposition d’une solution pertinente et la qualité du code Python d’implémentation de la solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyser le problème :\n",
    "- Tout d'abord, comment déterminer les critères de R, notamment **R***, autrement dit, comment devrait-on subdiviser les notes de contenu.\n",
    "- Dans quel aspects on peut trouver des sim. et diff.  entre R et R* ?\n",
    "- Comment fusionner R et R* pour générer une nouvelle réponse.\n",
    "- Comment évaluer le(s) nouvelle(s) réponse(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1️⃣ Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer les données\n",
    "prompts_train_df = pd.read_csv('data/prompts_train.csv')\n",
    "reponses_train_df = pd.read_csv('data/summaries_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7165, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vérifier la taille des données\n",
    "reponses_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000e8c3c7ddb</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The third wave was an experimentto see how peo...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0020ae56ffbf</td>\n",
       "      <td>ebad26</td>\n",
       "      <td>They would rub it up with soda to make the sme...</td>\n",
       "      <td>-0.548304</td>\n",
       "      <td>0.506755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004e978e639e</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>In Egypt, there were many occupations and soci...</td>\n",
       "      <td>3.128928</td>\n",
       "      <td>4.231226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>005ab0199905</td>\n",
       "      <td>3b9047</td>\n",
       "      <td>The highest class was Pharaohs these people we...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.471415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0070c9e7af47</td>\n",
       "      <td>814d6b</td>\n",
       "      <td>The Third Wave developed  rapidly because the ...</td>\n",
       "      <td>3.272894</td>\n",
       "      <td>3.219757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id                                               text  \\\n",
       "0  000e8c3c7ddb    814d6b  The third wave was an experimentto see how peo...   \n",
       "1  0020ae56ffbf    ebad26  They would rub it up with soda to make the sme...   \n",
       "2  004e978e639e    3b9047  In Egypt, there were many occupations and soci...   \n",
       "3  005ab0199905    3b9047  The highest class was Pharaohs these people we...   \n",
       "4  0070c9e7af47    814d6b  The Third Wave developed  rapidly because the ...   \n",
       "\n",
       "    content   wording  \n",
       "0  0.205683  0.380538  \n",
       "1 -0.548304  0.506755  \n",
       "2  3.128928  4.231226  \n",
       "3 -0.210614 -0.471415  \n",
       "4  3.272894  3.219757  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vérifier les premières lignes des données pour avoir une idée de la structure\n",
    "reponses_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2️⃣ EDA (Exploratory Data Analysis) et Ingénierie des caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "réponses uniques: 7165\n",
      "prompts uniques: 4\n",
      "******************************************************************************************\n",
      "Statistiques de score de content :\n",
      "count    7165.000000\n",
      "mean       -0.014853\n",
      "std         1.043569\n",
      "min        -1.729859\n",
      "25%        -0.799545\n",
      "50%        -0.093814\n",
      "75%         0.499660\n",
      "max         3.900326\n",
      "Name: content, dtype: float64\n",
      "******************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# obtenir le nombre de réponses uniques et de prompts uniques\n",
    "unique_rep = reponses_train_df['student_id'].nunique()\n",
    "unique_prompts = reponses_train_df['prompt_id'].nunique()\n",
    "\n",
    "# obtenir les données statistiques descriptives sur les scores de \"content\".\n",
    "content_score_stats = reponses_train_df['content'].describe()\n",
    "print('réponses uniques:', unique_rep)\n",
    "print('prompts uniques:', unique_prompts)\n",
    "print('*'*90)\n",
    "print('Statistiques de score de content :')\n",
    "print(content_score_stats)\n",
    "print('*'*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Un grand nombre d'élèves ont des scores négatifs.\n",
    "2. Il existe des réponses de qualité.\n",
    "3. Une diversité de réponses en termes de qualité de contenu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va ensuite effectuer les opérations suivantes sur les deux DataFrames `prompt_train_df`:\n",
    "1. Fusionner les 2 tableaux\n",
    "2. Supprimer la colonne \"wording\"\n",
    "3. Ajouter d'autres caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并两个DataFrame\n",
    "merged_df = reponses_train_df.merge(prompts_train_df[['prompt_id', 'prompt_text','prompt_question']], on='prompt_id', how='left')\n",
    "\n",
    "\n",
    "#Text transformation\n",
    "#将merged_df的字符串内容转换为小写\n",
    "merged_df = merged_df.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "\n",
    "# 确保所有的文本都是字符串\n",
    "merged_df[\"text\"]=[str(data) for data in merged_df.text] #converting all to string\n",
    "merged_df[\"prompt_text\"]=[str(data) for data in merged_df.prompt_text] #converting all to string\n",
    "\n",
    "# #删除所有标点符号\n",
    "# merged_df[\"text\"]=merged_df.text.apply(lambda x: re.sub('[^A-Za-z0-9 ]+', ' ', x))\n",
    "# merged_df[\"prompt_text\"]=merged_df.prompt_text.apply(lambda x: re.sub('[^A-Za-z0-9 ]+', ' ', x))\n",
    "# 没有删除标点符号，因为标点符号对于一些引用的情况是有用的\n",
    "\n",
    "\n",
    "# 删除wording列，聚焦于 content 评分\n",
    "merged_df = merged_df.drop(columns=['wording'])\n",
    "\n",
    "merged_df.head()\n",
    "\n",
    "merged_df.to_csv('data/tood.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = SentenceTransformer('sentence-transformers/all-distilroberta-v1', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_to_emb_dict = merged_df.groupby('prompt_id')['prompt_text'].first().transform(lambda x: embeddings_model.encode(x, batch_size=1, show_progress_bar=False)).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity(row, model=embeddings_model, prompt_embeddings=prompt_to_emb_dict):\n",
    "    prompt_vector = prompt_embeddings[row['prompt_id']]\n",
    "    summary_vector = model.encode(row['text'], batch_size=1, show_progress_bar=False)\n",
    "    return util.cos_sim([prompt_vector], [summary_vector]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['semantic_similarity'] = merged_df.apply(semantic_similarity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加新 feature，获取 reponse 的长度以及 prompt 的长度，添加到 dataframe 中\n",
    "\n",
    "def feature_engineering(df):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    def count_text_length(df, col, tokenizer):\n",
    "        longueur_col = df[col].apply(lambda x: len(tokenizer.encode(x)))\n",
    "        return longueur_col\n",
    "    \n",
    "    df['reponse_longueur'] = count_text_length(df, 'text',tokenizer)\n",
    "    df['prompt_longueur'] = count_text_length(df, 'prompt_text',tokenizer)\n",
    "    df['rep/prompt_ratio'] = df['reponse_longueur'] / df['prompt_longueur']\n",
    "    df['vocabulary_richness'] = df['text'].apply(lambda x: len(set(x.split())))\n",
    "    df['word_overlap'] = df.apply(lambda row: len(set(row['prompt_text'].split()) & set(row['text'].split())), axis=1)\n",
    "    \n",
    "    def quotes_count(row):\n",
    "        summary = row['text']\n",
    "        text    = row['prompt_text']\n",
    "        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n",
    "        return [quote in text for quote in quotes_from_summary].count(True)\n",
    "\n",
    "    df['quotes_num'] = df.apply(quotes_count, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    def word_ngram_overlap(row, n):\n",
    "        original_tokens = row['prompt_text'].split()\n",
    "        summary_tokens = row['text'].split()\n",
    "\n",
    "        original_ngrams = set(ngrams(original_tokens, n))\n",
    "        summary_ngrams = set(ngrams(summary_tokens, n))\n",
    "        \n",
    "        common_ngrams = original_ngrams.intersection(summary_ngrams)\n",
    "        \n",
    "        return len(common_ngrams) / len(summary_ngrams) if len(summary_ngrams) else 0, len(common_ngrams)\n",
    "\n",
    "\n",
    "    df['bigram_overlap']  = df.apply(lambda x: word_ngram_overlap(x,2)[1], axis=1 )\n",
    "    df['trigram_overlap'] = df.apply(lambda x: word_ngram_overlap(x,3)[1], axis=1 )\n",
    "    df['jaccard_similarity'] = df.apply(lambda row: len(set(word_tokenize(row['prompt_text'])) & set(word_tokenize(row['text']))) / len(set(word_tokenize(row['prompt_text'])) | set(word_tokenize(row['text']))), axis=1)\n",
    "\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_featured = feature_engineering(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_featured.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存这些特征\n",
    "merged_df_featured.to_csv('data/merged_df_featured.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the figure and axes for two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6,3), dpi=100)\n",
    "\n",
    "# Scatter plot for content score vs summary word count\n",
    "ax1.scatter(merged_df_featured['reponse_longueur'], merged_df_featured['content'], alpha=0.5, color='blue')\n",
    "ax1.set_title('S_content vs reponse_longueur')\n",
    "ax1.set_xlabel('reponse_longueur')\n",
    "ax1.set_ylabel('Content Score')\n",
    "\n",
    "# Scatter plot for content score vs unique word count\n",
    "ax2.scatter(merged_df_featured['vocabulary_richness'], merged_df_featured['content'], alpha=0.5, color='blue')\n",
    "ax2.set_title('S_content vs vocabulaire_longueur')\n",
    "ax2.set_xlabel('vocabulaire_longueur')\n",
    "ax2.set_ylabel('Content Score')\n",
    "\n",
    "# Adjusting the layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Displaying the plots\n",
    "plt.show()\n",
    "\n",
    "# Calculating the correlation between summary word count and content score\n",
    "word_count_content_corr = merged_df_featured['reponse_longueur'].corr(merged_df_featured['content'])\n",
    "\n",
    "# Calculating the correlation between unique word count and content score\n",
    "vocabulary_richness_corr = merged_df_featured['vocabulary_richness'].corr(merged_df_featured['content'])\n",
    "\n",
    "(word_count_content_corr, vocabulary_richness_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 观察结果：内容得分与摘要字数之间存在明显的正相关关系，表明较长的摘要往往会获得较高的内容得分。相关系数约为0.793。\n",
    "2. 观察结果：\n",
    "内容得分与vocabulaire 规模之间的关系：\n",
    "\n",
    "总结中使用的独特词汇数量与内容得分之间存在强正相关，表明使用更多种类的词汇的总结往往会获得较高的内容得分。相关系数约为0.807。而且这种相关系数比单纯的计算长度更加强，这表明使用的词汇种类的数量相比于单独的长度增加，对内容得分的影响，更大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "selected_vars = ['content', 'reponse_longueur', 'prompt_longueur', 'rep/prompt_ratio', 'vocabulary_richness', 'word_overlap', 'quotes_num', 'bigram_overlap', 'trigram_overlap', 'jaccard_similarity']\n",
    "\n",
    "# Create a subset DataFrame with selected variables\n",
    "subset_df = merged_df[selected_vars]\n",
    "\n",
    "# Calculate the correlation matrix for the subset\n",
    "correlation_matrice = subset_df.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "\n",
    "# Create a mask to hide values below the threshold\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "sns.heatmap(correlation_matrice, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Matrice des variables sélectionnées\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下一步：确定 R 和 R*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setting up the figure and axes\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "# Plotting histograms\n",
    "ax[0].hist(reponses_train_df['content'], bins=30, color='green', edgecolor='black')\n",
    "ax[0].set_title('Content Score Distribution')\n",
    "ax[0].set_xlabel('Content Score')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "\n",
    "ax[1].hist(merged_df['reponse_longueur'], bins=30, color='blue', edgecolor='black')\n",
    "ax[1].set_title('Summary Word Count Distribution')\n",
    "ax[1].set_xlabel('Word Count')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "\n",
    "# Adjusting the layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Displaying the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "内容得分分布：\n",
    "\n",
    "该分布略微呈左偏态，但基本符合正态分布，峰值稍低于零。这表明有相当数量的摘要获得了负面的内容得分。\n",
    "\n",
    "摘要字数分布：\n",
    "\n",
    "该分布高度右偏，说明大多数摘要包含较少的单词数量，只有少数摘要具有非常高的字数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean       -0.014853\n",
    "std         1.043569\n",
    "min        -1.729859\n",
    "25%        -0.799545\n",
    "50%        -0.093814\n",
    "75%         0.499660\n",
    "max         3.900326\n",
    "\n",
    "因此，结合上面的 content 分数分布图，以及之前的统计数据，我们认为，如果要判断一个回答的质量，可以对整个数据集进行划分：\n",
    "\n",
    "1. 较差回答 n <= -1\n",
    "2. 一般回答 -1 < n < 1\n",
    "3. 较好回答 1 <= n <= 3\n",
    "4. 极好回答 n > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_score(n):\n",
    "    if n <= 0:\n",
    "        return 'mauvaise réponse'\n",
    "    elif n <= 1:\n",
    "        return 'réponse moyenne'\n",
    "    elif n <= 3:\n",
    "        return 'bonne réponse'\n",
    "    else: # n > 3\n",
    "        return 'excellente réponse'\n",
    "\n",
    "# 应用这个函数到 'content' 列，并创建一个新列 'quality_category'\n",
    "merged_df['quality_category'] = merged_df['content'].apply(categorize_score)\n",
    "\n",
    "# 显示更新后的 DataFrame\n",
    "merged_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the counts of each category\n",
    "category_counts = merged_df['quality_category'].value_counts()\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来，抽取一个来自“bonne réponse”或“excellente回答”的学生的一些摘要，再抽取一个随机学生的摘要。\n",
    "\n",
    "\n",
    "# 假设 merged_df 是你的原始 DataFrame\n",
    "\n",
    "\n",
    "# 首先，我们从 '较好回答' 或 '极好回答' 中随机抽取一个记录\n",
    "good_answers_df = merged_df[merged_df['quality_category'].isin(['bonne réponse', 'excellente réponse'])]\n",
    "selected_good_answer = good_answers_df.sample(n=1)\n",
    "\n",
    "# 接下来，根据selected_good_answer的 prompt_id 从 prompts_train_df 中找到相应的所以记录\n",
    "# 然后，从相通 prompt_id 的数据集中随机抽取另一个记录（确保与第一个记录不同）\n",
    "selected_random_answer = merged_df[merged_df['prompt_id'] == selected_good_answer['prompt_id'].values[0]].drop(selected_good_answer.index).sample(n=1)\n",
    "\n",
    "# 将这两个记录分别生成新的 DataFrame\n",
    "df_good_answer = pd.DataFrame(selected_good_answer)\n",
    "df_random_answer = pd.DataFrame(selected_random_answer)\n",
    "\n",
    "\n",
    "\n",
    "# 从原始 df 中删除这两个记录\n",
    "merged_df = merged_df.drop(selected_good_answer.index)\n",
    "merged_df = merged_df.drop(selected_random_answer.index)\n",
    "\n",
    "\n",
    "# 显示结果\n",
    "df_good_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相同点通过 word embedding 或者 tfidf 提取关键词进行陈述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 df_good_answer 和 df_random_answer 中的 text 字段提取出来，并转换成 JSON 形式\n",
    "\n",
    "# 假设 df_good_answer 和 df_random_answer 已经正确定义\n",
    "# 提取 text 字段\n",
    "good_answer_text = df_good_answer['text'].iloc[0]\n",
    "random_answer_text = df_random_answer['text'].iloc[0]\n",
    "\n",
    "# 将文本转换成 JSON 格式\n",
    "texts_json_str = f'''{{\n",
    "    \"A\": \"{good_answer_text}\",\n",
    "    \"B\": \"{random_answer_text}\"\n",
    "}}'''\n",
    "\n",
    "\n",
    "texts_json_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API 来融合 R 和 R*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# use json to format the prompt\n",
    "texts = texts_json_str\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-0301\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \n",
    "     \"Given 2 texts: A and B in a json format . You need to intergrate the key points of A into B without changing too much B's content, in order to return a new text.\"},\n",
    "    {\"role\": \"system\", \"content\": \n",
    "     \"The length of the new text created should be less than the sum of the lengths of two texts. Your output should be in a correct json format, with the key 'combined_text' and the value as the new text.\"},\n",
    "    {\"role\": \"system\", \"content\": \n",
    "     \"You should focus on the contenu\"},\n",
    "    {\"role\": \"user\", \n",
    "     \"content\": texts}\n",
    "  ],\n",
    "    temperature=0.3\n",
    "    )\n",
    "# print(completion.choices[0].message)\n",
    "# print(dict(completion).get('usage'))\n",
    "\n",
    "data = completion.model_dump_json(indent=2)\n",
    "json_data = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = json_data['choices'][0]['message']['content']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returned_text = json.loads(text)\n",
    "print(returned_text)\n",
    "\n",
    "print(returned_text[\"combined_text\"])\n",
    "print(len(returned_text[\"combined_text\"]))\n",
    "print(len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 创建一个字典，包含学生ID、提示ID和文本字段的值\n",
    "stu_id= '000000ffffff'\n",
    "prompt_id = 'def789'\n",
    "text_test = returned_text[\"combined_text\"]\n",
    "\n",
    "data_4_test = {\n",
    "    'student_id': [stu_id],\n",
    "    'prompt_id': [prompt_id],\n",
    "    'text': [text_test]\n",
    "}\n",
    "\n",
    "# 使用字典创建DataFrame\n",
    "test_data = pd.DataFrame(data_4_test)\n",
    "\n",
    "# 打印DataFrame\n",
    "test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'prompts_train shape: {prompts_train_df.shape}')\n",
    "print(f'summaries_train shape: {merged_df.shape}')\n",
    "print('-'*90)\n",
    "print(f'prompts_train missing values: {prompts_train_df.isnull().sum().sum()}')\n",
    "print(f'summaries_train missing values: {merged_df.isnull().sum().sum()}')\n",
    "print('-'*90)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "def extract_features_ngrams(texts):\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    \n",
    "    features = tfidf_vectorizer.fit_transform(texts)\n",
    "    with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "        pickle.dump(tfidf_vectorizer, f)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_features_features(merged_df):\n",
    "    feature_columns = ['reponse_longueur', 'rep/prompt_ratio', 'vocabulary_richness', 'word_overlap', 'jaccard_similarity', 'quote_count']\n",
    "    train_features = merged_df[feature_columns]\n",
    "    return train_features\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def extract_features_tfidf(texts):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    features = tfidf_vectorizer.fit_transform(texts)\n",
    "    with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "        pickle.dump(tfidf_vectorizer, f)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = extract_features_features(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_summaries = merged_df.text\n",
    "test_text_summaries = test_data.text\n",
    "train_text_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_summaries_train = [preprocess_text(summary) for summary in train_text_summaries]\n",
    "preprocessed_summaries_test = [preprocess_text(summary) for summary in test_text_summaries]\n",
    "\n",
    "train_tfidf_features = extract_features_tfidf(preprocessed_summaries_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    loaded_tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "test_tfidf_features = loaded_tfidf_vectorizer.transform(preprocessed_summaries_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = merged_df[['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = XGBRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(new_features, target_labels, test_size=0.001, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regressor.predict(X_test)\n",
    "print(predictions)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, predictions, multioutput='raw_values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions = regressor.predict(test_tfidf_features)\n",
    "print(new_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def count_total_words(text: str) -> int:\n",
    "    words = text.split()\n",
    "    total_words = len(words)\n",
    "    return total_words\n",
    "\n",
    "def count_stopwords(text: str) -> int:\n",
    "    stopword_list = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    stopwords_count = sum(1 for word in words if word.lower() in stopword_list)\n",
    "    return stopwords_count\n",
    "\n",
    "def count_punctuation(text: str) -> int:\n",
    "    punctuation_set = set(string.punctuation)\n",
    "    punctuation_count = sum(1 for char in text if char in punctuation_set)\n",
    "    return punctuation_count\n",
    "\n",
    "def count_numbers(text: str) -> int:\n",
    "    numbers = re.findall(r'\\d+', text)\n",
    "    numbers_count = len(numbers)\n",
    "    return numbers_count\n",
    "\n",
    "def feature_engineer(dataframe: pd.DataFrame, feature: str = 'text') -> pd.DataFrame:\n",
    "    dataframe[f'{feature}_length'] = dataframe[feature].apply(lambda x: len(x))\n",
    "    dataframe[f'{feature}_word_cnt'] = dataframe[feature].apply(lambda x: count_total_words(x))\n",
    "    dataframe[f'{feature}_stopword_cnt'] = dataframe[feature].apply(lambda x: count_stopwords(x))\n",
    "    dataframe[f'{feature}_punct_cnt'] = dataframe[feature].apply(lambda x: count_punctuation(x))\n",
    "    dataframe[f'{feature}_number_cnt'] = dataframe[feature].apply(lambda x: count_numbers(x))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_train = feature_engineer(summaries_train)\n",
    "summaries_test = feature_engineer(summaries_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
